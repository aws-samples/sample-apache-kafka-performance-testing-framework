{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka Performance Testing Framework - Create Visualization\n",
    "\n",
    "This notebook provides visualization capabilities for analyzing Apache Kafka performance test results.\n",
    "\n",
    "The first cell sets up the required dependencies and AWS service clients for the performance analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and Utility Imports\n",
    "import boto3\n",
    "\n",
    "# Custom Module Imports\n",
    "from utils import (\n",
    "    get_test_details,\n",
    "    query_experiment_details,\n",
    "    aggregate_statistics,\n",
    "    plot\n",
    ")\n",
    "\n",
    "# AWS Client Initialization\n",
    "stepfunctions = boto3.client('stepfunctions')\n",
    "cloudwatch_logs = boto3.client('logs')\n",
    "cloudformation = boto3.client('cloudformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Parameters Configuration\n",
    "\n",
    "The next cell initializes the parameters needed to identify and retrieve test execution data from AWS Step Functions.\n",
    "\n",
    "### Usage\n",
    "Replace `'your-execution-arn-here'` with the actual ARN of your Step Functions execution. To analyze multiple test executions, add more dictionaries to the list:\n",
    "\n",
    "```python\n",
    "test_params.extend([\n",
    "    {'execution_arn': 'arn-1'},\n",
    "    {'execution_arn': 'arn-2'},\n",
    "    # Add more ARNs as needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters configuration\n",
    "test_params = []\n",
    "test_params.extend([\n",
    "    {'execution_arn': 'your-execution-arn-here'}  # Replace with your actual ARN\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Details Retrieval\n",
    "\n",
    "This cell fetches detailed information about the Kafka performance tests using the provided execution ARNs.\n",
    "\n",
    "The output provides a comprehensive view of both the cluster configuration and the performance test parameters in a well-organized, readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test details\n",
    "try:\n",
    "    test_details = get_test_details.get_test_details(test_params, stepfunctions, cloudformation)\n",
    "except Exception as e:\n",
    "    print(f\"Error getting test details: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Experiment Details\n",
    "\n",
    "This cell retrieves and processes performance statistics from CloudWatch Logs for the Kafka performance tests.\n",
    "\n",
    "The output provides the total number of statistics gathered for both producers and consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query experiment details\n",
    "producer_stats, consumer_stats, test_details = query_experiment_details.query_cw_logs(\n",
    "    test_details,\n",
    "    cloudwatch_logs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Aggregation\n",
    "\n",
    "This cell prepares the data for visualization by defining visualization parameters, setting up filters, and aggregating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define partitions for the visualization\n",
    "partitions = {\n",
    "    'ignore_keys': [\n",
    "        'topic_id', \n",
    "        'cluster_id', \n",
    "        'test_id', \n",
    "        'cluster_name'\n",
    "    ],\n",
    "    'title_keys': [\n",
    "        'kafka_version',\n",
    "        'broker_storage',\n",
    "        'in_cluster_encryption',\n",
    "        'producer.security.protocol'\n",
    "    ],\n",
    "    'row_keys': [\n",
    "        'producer.acks',\n",
    "        'producer.batch.size',\n",
    "        'num_partitions'\n",
    "    ],\n",
    "    'column_keys': [\n",
    "        'num_producers',\n",
    "        'consumer_groups.num_groups'\n",
    "    ],\n",
    "    'metric_color_keys': [\n",
    "        'broker_type'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define filter functions\n",
    "filter_fn = lambda x: True\n",
    "filter_agg_fn = lambda x: True\n",
    "\n",
    "# Apply filters and aggregate data\n",
    "filtered_producer_stats = list(filter(filter_fn, producer_stats))\n",
    "filtered_consumer_stats = list(filter(filter_fn, consumer_stats))\n",
    "\n",
    "(producer_aggregated_stats, consumer_aggregated_stats, combined_stats) = aggregate_statistics.aggregate_cw_logs(\n",
    "    filtered_producer_stats, \n",
    "    filtered_consumer_stats, \n",
    "    partitions,\n",
    "    test_details\n",
    ")\n",
    "\n",
    "filtered_producer_aggregated_stats = list(filter(filter_agg_fn, producer_aggregated_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Visualization\n",
    "\n",
    "This cell creates a visualization of producer latency metrics across different throughput levels.\n",
    "\n",
    "This visualization helps identify how producer latency changes as throughput increases, revealing potential performance bottlenecks and the relationship between typical and worst-case latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "if filtered_producer_aggregated_stats:\n",
    "    plot.plot_measurements(\n",
    "        filtered_producer_aggregated_stats,\n",
    "        ['latency_ms_p50', 'latency_ms_p99'],\n",
    "        'producer put latency (ms)',\n",
    "        xlogscale=True,\n",
    "        ylogscale=True,\n",
    "        xmin=1,\n",
    "        ymin=1,\n",
    "        **partitions\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo data available for plotting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Visualization\n",
    "\n",
    "This cell creates a visualization focusing on the actual throughput achieved by Kafka producers.\n",
    "\n",
    "This visualization complements the latency plot by showing the throughput dimension of performance, allowing for a complete understanding of the throughput-latency tradeoff in Kafka deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create throughput visualization\n",
    "if filtered_producer_aggregated_stats:\n",
    "    plot.plot_measurements(\n",
    "        filtered_producer_aggregated_stats,\n",
    "        ['sent_mb_sec'],\n",
    "        'producer throughput (MB/s)',\n",
    "        xlogscale=True,\n",
    "        ylogscale=True,\n",
    "        xmin=1,\n",
    "        ymin=1,\n",
    "        **partitions\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nNo data available for plotting!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
